from flask import Flask, render_template, request, jsonify
import os
import sys
import shutil
import json
import csv
import subprocess
from datetime import datetime
import config as cfg

import unicodedata

def normalize_text(text: str) -> str:
    text_norm = unicodedata.normalize('NFD', text)
    stripped = ''.join(
        c for c in text_norm if unicodedata.category(c) != 'Mn'
    )
    return stripped.lower()


app = Flask(__name__)

# ---- Global State ----
SCRAPER_PROCESS = None  # Subprocess de Scrapy

# ---- Helpers ----

def timestamp_dir():
    return datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

def copy_if_exists(src, dst_dir):
    if os.path.isfile(src):
        shutil.copy2(src, os.path.join(dst_dir, os.path.basename(src)))

# ---- Rutas de la API ----

@app.route('/api/files/<filename>', methods=['GET'])
def get_file(filename):
    # Validar que solo se pueda acceder a archivos permitidos
    allowed_files = {
        'fuentes.csv': cfg.FUENTES_FILE,
        'exclusiones.txt': cfg.EXCLUSIONES_FILE,
        'terminos_interes.txt': cfg.TERMINOS_FILE,
    }
    filepath = allowed_files.get(filename)
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'File not found'}), 404

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        return jsonify({'content': content})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/files/<filename>', methods=['POST'])
def save_file(filename):
    allowed_files = {
        'fuentes.csv': cfg.FUENTES_FILE,
        'exclusiones.txt': cfg.EXCLUSIONES_FILE,
        'terminos_interes.txt': cfg.TERMINOS_FILE,
    }
    filepath = allowed_files.get(filename)
    if not filepath:
        return jsonify({'error': 'File not allowed'}), 400

    data = request.json
    if 'content' not in data:
        return jsonify({'error': 'Missing content'}), 400

    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(data['content'])
        return jsonify({'success': True})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/scrape', methods=['POST'])
def start_scrape():
    global SCRAPER_PROCESS
    # Verificar si ya hay un proceso corriendo
    if SCRAPER_PROCESS and SCRAPER_PROCESS.poll() is None:
        return jsonify({'error': 'Un proceso de scraping ya está en ejecución.'}), 409

    # 1. Leer configuración del usuario desde el request
    user_config = request.json or {}

    # 2. Crear directorios de ejecución
    exec_name = timestamp_dir()
    current_execution_dir = os.path.join(cfg.EJECUCIONES_DIR, exec_name)
    os.makedirs(current_execution_dir)

    current_documents_dir = os.path.join(current_execution_dir, "autoconsumo_documents")
    os.makedirs(current_documents_dir)

    # 3. Copiar ficheros de entrada
    copy_if_exists(cfg.FUENTES_FILE, current_execution_dir)
    copy_if_exists(cfg.EXCLUSIONES_FILE, current_execution_dir)
    copy_if_exists(cfg.TERMINOS_FILE, current_execution_dir)

    # 4. Guardar configuración en JSON para que el script de Scrapy la lea
    config_file = os.path.join(current_execution_dir, 'scraper_config.json')
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump({
            'execution_dir': current_execution_dir,
            'documents_dir': current_documents_dir,
            'fuentes_file': cfg.FUENTES_FILE,
            'terminos_file': cfg.TERMINOS_FILE,
            'exclusiones_file': cfg.EXCLUSIONES_FILE,
            'user_config': user_config
        }, f, indent=2)

    # 5. Ejecutar Scrapy en un proceso separado
    script_path = os.path.join(cfg.BASE_DIR, 'run_scraper.py')

    try:
        SCRAPER_PROCESS = subprocess.Popen(
            [sys.executable, script_path, config_file],
            cwd=cfg.BASE_DIR,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        return jsonify({'message': f'Scraping iniciado en la carpeta de ejecución: {exec_name}'})
    except Exception as e:
        return jsonify({'error': f'Error al iniciar el scraping: {e}'}), 500

# ... (resto de las rutas sin cambios por ahora)

@app.route('/api/scrape/cancel', methods=['POST'])
def cancel_scrape():
    return jsonify({'error': 'La cancelación no está soportada en esta versión.'}), 501

@app.route('/api/scrape_status', methods=['GET'])
def scrape_status():
    status_path = os.path.join(cfg.BASE_DIR, 'status.json')
    if not os.path.exists(status_path):
        # Si el fichero no existe, se asume que no hay scraping
        return jsonify({'status': 'idle', 'current': 0, 'total': 0})
    try:
        with open(status_path, 'r', encoding='utf-8') as f:
            status = json.load(f)
        # Comprobar si el proceso sigue vivo (subprocess)
        is_crawling = False
        if SCRAPER_PROCESS:
            is_crawling = SCRAPER_PROCESS.poll() is None  # None = aún corriendo

        if status.get('status') == 'running' and not is_crawling:
            status['status'] = 'idle' # Marcar como idle si el proceso ha terminado
            with open(status_path, 'w', encoding='utf-8') as f:
                json.dump(status, f)
        return jsonify(status)
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/procesados', methods=['GET'])
def get_procesados():
    # Encontrar la ejecución más reciente
    if not os.path.exists(cfg.EJECUCIONES_DIR) or not os.listdir(cfg.EJECUCIONES_DIR):
        return jsonify({'content': ''})

    try:
        latest_exec = sorted(os.listdir(cfg.EJECUCIONES_DIR), reverse=True)[0]
        latest_exec_path = os.path.join(cfg.EJECUCIONES_DIR, latest_exec)
        filepath = os.path.join(latest_exec_path, 'procesados.md')

        if not os.path.exists(filepath):
            return jsonify({'content': f'# Ejecución: {latest_exec}\n\n*Procesando...*'}) # Devuelve vacío si no existe

        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        # Añadir el nombre de la ejecución al contenido
        content_with_header = f'# Ejecución: {latest_exec}\n\n{content}'
        return jsonify({'content': content_with_header})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/logs', methods=['GET'])
def get_logs():
    if not os.path.exists(cfg.EJECUCIONES_DIR) or not os.listdir(cfg.EJECUCIONES_DIR):
        return jsonify({'content': 'Aún no hay ejecuciones.'})

    try:
        latest_exec = sorted(os.listdir(cfg.EJECUCIONES_DIR), reverse=True)[0]
        latest_exec_path = os.path.join(cfg.EJECUCIONES_DIR, latest_exec)
        log_filepath = os.path.join(latest_exec_path, 'scraper.log')

        if not os.path.exists(log_filepath):
            return jsonify({'content': 'Esperando inicio del scraper...'}) # Devuelve vacío si no existe

        with open(log_filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        return jsonify({'content': content})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ---- Ruta de la página principal ----

@app.route('/')
def index():
    return render_template('index.html')

if __name__ == '__main__':
    app.run(debug=True, port=5001)
