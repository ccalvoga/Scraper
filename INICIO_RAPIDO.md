# ğŸš€ Inicio RÃ¡pido - Web Scraper

## â–¶ï¸ Iniciar la AplicaciÃ³n

### Paso 1: Ejecutar Flask
```bash
python app.py
```

**Salida esperada**:
```
 * Serving Flask app 'app'
 * Debug mode: on
WARNING: This is a development server. Do not use it in a production deployment.
 * Running on http://127.0.0.1:5001
Press CTRL+C to quit
```

### Paso 2: Abrir Navegador
```
http://localhost:5001
```

---

## ğŸ¯ Uso BÃ¡sico (Primeros Pasos)

### 1. Verificar Archivos de Entrada

AsegÃºrate de que existan estos archivos:

**fuentes.csv** (URLs a scrapear):
```csv
descripcion;URL
ejemplo;https://www.miteco.gob.es/es/energia/renovables/regimen-economico-energias-renovables.html
```

**terminos_interes.txt** (Palabras clave):
```
autoconsumo
renovables
fotovoltaica
```

**exclusiones.txt** (Opcional - tÃ©rminos a evitar):
```
error
404
```

---

### 2. Iniciar Scraping Simple

**OpciÃ³n A: Con configuraciÃ³n por defecto**
1. Click en **"Iniciar Scraping"**
2. Esperar a que termine
3. Ver resultados en panel de logs

**OpciÃ³n B: Con configuraciÃ³n personalizada**
1. Click en **"âš™ï¸ ConfiguraciÃ³n Avanzada"**
2. Ajustar opciones (profundidad, tipos de archivos, etc.)
3. Click en **"âœ“ Aplicar ConfiguraciÃ³n"**
4. Click en **"Iniciar Scraping"**

---

### 3. Monitorear Progreso

La interfaz muestra en tiempo real:
- **Logs** â†’ Panel derecho (scroll automÃ¡tico)
- **Estado** â†’ Mensaje junto a botones
- **Resultados** â†’ Panel inferior (se actualiza cada 5 segundos)

---

### 4. Ver Resultados

Los archivos descargados estÃ¡n en:
```
ejecuciones/
  â””â”€â”€ 2025-11-03_22-30-00/        â† Carpeta timestamped
      â”œâ”€â”€ autoconsumo_documents/   â† Archivos descargados
      â”‚   â”œâ”€â”€ pagina1.html.txt     â† Texto extraÃ­do
      â”‚   â”œâ”€â”€ pagina1.html         â† HTML original
      â”‚   â”œâ”€â”€ documento.pdf        â† PDFs descargados
      â”‚   â””â”€â”€ ...
      â”œâ”€â”€ scraper.log              â† Log completo
      â”œâ”€â”€ scraper_config.json      â† ConfiguraciÃ³n usada
      â”œâ”€â”€ fuentes.csv              â† Copia de fuentes
      â”œâ”€â”€ terminos_interes.txt     â† Copia de tÃ©rminos
      â””â”€â”€ exclusiones.txt          â† Copia de exclusiones
```

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Opciones Disponibles

| OpciÃ³n | Valores | Por Defecto |
|--------|---------|-------------|
| **Profundidad MÃ¡xima** | 0-10 | 3 |
| **Comportamiento sin tÃ©rminos** | Continuar / Detener | Continuar |
| **Tipos de archivos** | Documents / Images / Archives / Other | Documents |
| **Alcance de descargas** | Mismo dominio / Cualquier dominio | Mismo dominio |
| **RestricciÃ³n de directorio** | Solo base path / Todo dominio | Solo base path |
| **Guardar texto** | SÃ­ / No | SÃ­ |
| **Guardar HTML** | SÃ­ / No | SÃ­ |

---

## â“ SoluciÃ³n de Problemas

### Problema: Flask no inicia
```bash
# Verificar que el puerto 5001 estÃ© libre
netstat -ano | findstr :5001

# Si estÃ¡ ocupado, cambiar puerto en app.py:
# app.run(debug=True, port=5002)
```

### Problema: No encuentra fuentes.csv
```bash
# Crear archivo de ejemplo
echo "ejemplo;https://www.example.com" > fuentes.csv
```

### Problema: Scraping no inicia
1. Ver logs en panel derecho
2. Verificar que `run_scraper.py` existe
3. Verificar que las URLs en `fuentes.csv` son vÃ¡lidas

### Problema: No se descargan archivos
1. Abrir **ConfiguraciÃ³n Avanzada**
2. Verificar que "Documentos" estÃ© seleccionado
3. Aplicar configuraciÃ³n
4. Reiniciar scraping

---

## ğŸ“Š Ejemplos de ConfiguraciÃ³n

### Ejemplo 1: Scraping RÃ¡pido
```
Profundidad: 1
Comportamiento: Detener rama
Archivos: Solo documentos
RestricciÃ³n: Solo base path
```
â†’ Ideal para actualizaciÃ³n rÃ¡pida de contenido conocido

### Ejemplo 2: Scraping Profundo
```
Profundidad: 5
Comportamiento: Continuar
Archivos: Todos los tipos
RestricciÃ³n: Todo el dominio
```
â†’ Ideal para archivo completo de un sitio web

### Ejemplo 3: Solo Multimedia
```
Profundidad: 3
Comportamiento: Continuar
Archivos: ImÃ¡genes + Archivos comprimidos
Guardar texto: No
Guardar HTML: No
```
â†’ Ideal para descargar recursos multimedia

---

## ğŸ” Ver Logs Detallados

### Durante el scraping:
- **UI Web**: Panel de logs en tiempo real
- **Archivo**: `ejecuciones/{timestamp}/scraper.log`

### DespuÃ©s del scraping:
```bash
# Ver Ãºltimas 50 lÃ­neas del log
powershell -Command "Get-Content 'ejecuciones\{timestamp}\scraper.log' -Tail 50"
```

---

## ğŸ›‘ Detener el Scraping

**Actualmente**:
1. Cerrar Flask (Ctrl+C en terminal)
2. El proceso de Scrapy terminarÃ¡ automÃ¡ticamente

**Futuro** (implementaciÃ³n pendiente):
- BotÃ³n "Cancelar" en la UI

---

## ğŸ“ Estructura de Directorios

```
04 WEB_Scraper/
â”œâ”€â”€ app.py                      â† Flask (servidor web)
â”œâ”€â”€ run_scraper.py              â† Scrapy (script de scraping)
â”œâ”€â”€ config.py                   â† ConfiguraciÃ³n general
â”œâ”€â”€ fuentes.csv                 â† URLs a scrapear
â”œâ”€â”€ terminos_interes.txt        â† Palabras clave
â”œâ”€â”€ exclusiones.txt             â† TÃ©rminos a evitar
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html              â† Interfaz web
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ style.css               â† Estilos
â”‚   â””â”€â”€ script.js               â† LÃ³gica frontend
â”œâ”€â”€ autoconsumo_scraper_scrapy/ â† Proyecto Scrapy
â”‚   â””â”€â”€ autoconsumo_scraper_scrapy/
â”‚       â”œâ”€â”€ spiders/
â”‚       â”‚   â””â”€â”€ generic_spider.py   â† Spider principal
â”‚       â”œâ”€â”€ pipelines.py        â† Guardado de archivos
â”‚       â”œâ”€â”€ items.py            â† Estructura de datos
â”‚       â””â”€â”€ settings.py         â† Config de Scrapy
â””â”€â”€ ejecuciones/                â† Resultados (timestamped)
```

---

## ğŸ“š DocumentaciÃ³n Completa

- `CONFIGURACION_AVANZADA.md` - ExplicaciÃ³n detallada de opciones (300+ lÃ­neas)
- `MEJORAS_MULTIDOMINIOS.md` - Soporte multi-dominio
- `FIX_STATUS_BREAKPOINT.md` - Arquitectura de procesos separados
- `FIX_THREADING_ERROR.md` - EvoluciÃ³n de la soluciÃ³n

---

## âœ… Checklist Antes de Scrapear

- [ ] Flask estÃ¡ corriendo (`python app.py`)
- [ ] `fuentes.csv` tiene URLs vÃ¡lidas
- [ ] `terminos_interes.txt` tiene palabras clave
- [ ] ConfiguraciÃ³n ajustada (si es necesario)
- [ ] Navegador abierto en `http://localhost:5001`

**Â¡Listo para scrapear!** ğŸš€

---

## ğŸ†˜ Ayuda

Si encuentras problemas:
1. Revisa los logs en la UI
2. Verifica `ejecuciones/{timestamp}/scraper.log`
3. Consulta la documentaciÃ³n en los archivos `.md`
4. Verifica que todos los archivos Python compilen sin errores

**Prueba rÃ¡pida de sintaxis**:
```bash
python -m py_compile app.py
python -m py_compile run_scraper.py
```

Ambos deben ejecutarse sin errores.
