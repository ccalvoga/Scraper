# Mejoras: Soporte Multi-Dominio

## üéØ Objetivo
Permitir que el scraper funcione con URLs de **m√∫ltiples dominios** diferentes, no solo `www.miteco.gob.es`.

## ‚ú® Cambios Realizados

### 1. **Nuevo Spider Gen√©rico** (`generic_spider.py`)
- **Antes**: `MitecoSpider` con `allowed_domains = ["www.miteco.gob.es"]` hardcodeado
- **Ahora**: `GenericSpider` que extrae dominios din√°micamente de las URLs

#### Extracci√≥n Din√°mica de Dominios
```python
# En __init__ del spider:
self.allowed_domains = list(set([urlparse(url).netloc for url in start_urls if url]))
```

**Ejemplo**:
Si `fuentes.csv` contiene:
```csv
descripcion1;https://www.miteco.gob.es/es/energia/renovables/
descripcion2;https://www.idae.es/tecnologias/energias-renovables
descripcion3;https://www.boe.es/buscar/act.php
```

El spider autom√°ticamente permitir√°:
- `www.miteco.gob.es`
- `www.idae.es`
- `www.boe.es`

### 2. **Actualizaci√≥n de `app.py`**
- Cambio del import: `MitecoSpider` ‚Üí `GenericSpider`
- Agregados imports faltantes: `sys`, `datetime`, `config as cfg`
- El resto de la l√≥gica permanece igual

### 3. **Mejora en la L√≥gica de Crawling**
```python
# Antes:
if target_domain == self.allowed_domains[0] and target_path.startswith(base_path):

# Ahora:
if target_domain in self.allowed_domains and target_path.startswith(base_path):
```

Esto permite que el spider:
- ‚úÖ Navegue por cualquier dominio que est√© en `fuentes.csv`
- ‚úÖ Respete la restricci√≥n de directorio base (no sale del path inicial)
- ‚úÖ Respete la profundidad m√°xima (`max_depth`)

## üìã Ejemplo de Uso

### Archivo `fuentes.csv`
```csv
faq regimen retribucion energias renovables;https://www.miteco.gob.es/es/energia/renovables/regimen-economico-energias-renovables.html
energia renovable idae;https://www.idae.es/tecnologias/energias-renovables
normativa autoconsumo;https://www.boe.es/buscar/act.php?id=BOE-A-2019-5089
subvenciones autoconsumo;https://www.comunidad.madrid/servicios/medio-ambiente/ayudas-instalaciones-autoconsumo
```

### Archivo `terminos_interes.txt`
```
autoconsumo
energ√≠a renovable
placas solares
fotovoltaica
subvenciones
```

### Archivo `exclusiones.txt`
```
error
p√°gina no encontrada
404
```

### Resultado
El scraper:
1. **Crawlear√°** todas las URLs de diferentes dominios
2. **Filtrar√°** p√°ginas que contengan palabras clave
3. **Excluir√°** ramas con t√©rminos prohibidos
4. **Descargar√°** PDFs y documentos relacionados
5. **Guardar√°** todo en `ejecuciones/{timestamp}/`

## üîß Configuraci√≥n

### Profundidad de Crawling
Por defecto: `max_depth=3`

Para cambiar, modificar en `app.py:129`:
```python
max_depth = request.json.get('max_depth', 3)  # Cambiar el 3
```

### Formatos de Archivo Soportados
```python
recognized_exts = {'.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx'}
```

Para agregar m√°s formatos, editar `generic_spider.py:76`.

## ‚ö†Ô∏è Restricciones Mantenidas

1. **Scope por Directorio**: Solo sigue enlaces dentro del directorio base
   - ‚úÖ `/es/energia/renovables/` ‚Üí `/es/energia/renovables/autoconsumo/`
   - ‚ùå `/es/energia/renovables/` ‚Üí `/es/agua/`

2. **Robots.txt**: Respeta las reglas de cada sitio

3. **Rate Limiting**:
   - 1 petici√≥n por dominio simult√°nea
   - 1 segundo de delay entre peticiones

## üìä Compatibilidad

### Compatible con:
- ‚úÖ M√∫ltiples dominios simult√°neos
- ‚úÖ HTTP y HTTPS
- ‚úÖ URLs con par√°metros (`?id=123`)
- ‚úÖ Subdominios diferentes (`www.example.com`, `blog.example.com`)

### No compatible con:
- ‚ùå Sitios con JavaScript rendering (SPA)
- ‚ùå Sitios que requieren autenticaci√≥n
- ‚ùå Contenido din√°mico cargado por AJAX

## üöÄ Pr√≥ximas Mejoras Potenciales

1. **Soporte para JavaScript**: Integrar Scrapy-Splash o Selenium
2. **Autenticaci√≥n**: Login forms y OAuth
3. **Base de Datos**: Guardar en PostgreSQL/SQLite
4. **Export Avanzado**: JSON, CSV, XML
5. **Notificaciones**: Email/Webhook al completar
6. **Dashboard**: Visualizaci√≥n de resultados en tiempo real
7. **Scheduler**: Ejecuciones programadas

## üìù Notas T√©cnicas

### Normalizaci√≥n de Texto
La funci√≥n `normalize_text()` maneja:
- Acentos: `energ√≠a` ‚Üí `energia`
- May√∫sculas: `RENOVABLE` ‚Üí `renovable`
- Diacr√≠ticos: `√±` se mantiene como `√±`

### Logs
Ubicaci√≥n: `ejecuciones/{timestamp}/scraper.log`

Ver en tiempo real:
```bash
GET /api/logs
```

### Estado del Scraper
```bash
GET /api/scrape_status
```

Respuesta:
```json
{
  "status": "running|idle|error",
  "current": 0,
  "total": 0
}
```
